<!DOCTYPE html>
<html lang="en-us">

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Hugo 0.62.0" />

  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="">
  <meta property="og:url" content="http://rlouf.github.com/post/jax-random-walk-metropolis/">

  <title>Massively parallel MCMC with JAX - /dev/null</title>
  <meta property="og:title" content="Massively parallel MCMC with JAX - /dev/null">
  <meta property="og:type" content="article">
  <meta name="description" content="">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Serif+Pro:400,700">
  <link rel="stylesheet" href="/css/highlight.css">
  <link rel="stylesheet" href="/css/journal.css">
  <link href="/index.xml" rel="alternate" type="application/rss+xml" title="/dev/null">

</head>

<body>
  <div class="container">

    <nav class="site-nav">
      <a href="http://rlouf.github.com/">Index</a>
    </nav>


  <article class="post">
    <header class="post-header">
      <h1 class="post-title">Massively parallel MCMC with JAX</h1>
      <time class="post-date" datetime="2019-01-09 13:19:11 CET">09 Jan 2019</time>
    </header>

    <h2 id="tldr">TL;DR</h2>
<p>JAX blows everyone out of the water, up to a factor of 20x in extreme cases
(1,000 samples with 1,000,000 chains). Numpy wins in the small number of
samples, small number of chains regime due to JAX's JIT compilation.  TFP with
XLA compilation shines in the small number of chains, large number of samples
regime. I must be doing something wrong with Tensorflow Probability as the
numbers I get otherwise are very underwhelming.</p>
<h2 id="introduction">Introduction</h2>
<p>I recently got very excited about the possibility to generate thousands,
millions of sampling chains with Monte Carlo algorithms. I vaguely followed
several discussions around the topic</p>
<p>I have seen here and there discussion about vectorized sampling algorithms that
were able to generate hundreds, thousands of chains in one pass.</p>
<p>Colin Caroll posted an interesting <a href="https://colindcarroll.com/2019/08/18/very-parallel-mcmc-sampling/">blog
post</a> that
uses numpy and a vectorized version of the random walk metropolis-hastings
(RWMH) algorithm to generate a large number of samples.</p>
<p>Around the same time I stumbled upon <a href="http://github.com/jax/jax">JAX</a>.  Jax
offers Just-in-Time compilation using <a href="">XLA</a>, inherits from autograd's autodiff
functionality. It favours a functional style of programming which makes sense in
a world where we are combining mathematical functions. I fell in love (there is
some place left for stickers on my laptop, btw!).</p>
<p>So, for future reference I decided to benchmark different backends:</p>
<ul>
<li>Numpy</li>
<li>Jax</li>
<li>Tensorflow Probability (TFP)</li>
<li>Tensorflow Probability with XLA compilation</li>
</ul>
<p>I am sampling an arbitrary Gaussian mixture with 4 components. Using Numpy:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> scipy.stats <span style="color:#f92672">import</span> norm

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mixture_logpdf</span>(x):
    <span style="color:#e6db74"></span><span style="color:#e6db74">&#34;&#34;&#34;</span><span style="color:#e6db74">Log probability distribution function of a gaussian mixture model.</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">    x: np.ndarray (4, n_chains)</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">        Position at which to evaluate the probability density function.</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">    </span><span style="color:#e6db74">&#34;&#34;&#34;</span>
    log_probs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(
        [
            norm(<span style="color:#f92672">-</span><span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">1.2</span>)<span style="color:#f92672">.</span>logpdf(x[<span style="color:#ae81ff">0</span>]),
            norm(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>logpdf(x[<span style="color:#ae81ff">1</span>]),
            norm(<span style="color:#ae81ff">3.2</span>, <span style="color:#ae81ff">5</span>)<span style="color:#f92672">.</span>logpdf(x[<span style="color:#ae81ff">2</span>]),
            norm(<span style="color:#ae81ff">2.5</span>, <span style="color:#ae81ff">2.8</span>)<span style="color:#f92672">.</span>logpdf(x[<span style="color:#ae81ff">3</span>]),
        ]
    )
    weights <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>repeat(np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.3</span>, <span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.4</span>]])<span style="color:#f92672">.</span>T, x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
    <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>logsumexp(np<span style="color:#f92672">.</span>log(weights) <span style="color:#f92672">-</span> log_probs, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</code></pre></div><p>The code for all 3 frameworks is available in this <a href="">gist</a>.</p>
<h2 id="notes-about-benchmarking">Notes about benchmarking</h2>
<p>Before giving the results, a few words of caution:</p>
<ol>
<li>The reported times are the average of 10 runs on my laptop, with nothing
other than the terminal open. For all but the post-compilation JAX runs,
the times were measure by the <code>hyperfine</code> command line tool.</li>
<li>My code is probably not optimal, especially for Numpy and TFP. I would
appreciate tips to make the codes faster.</li>
<li>The experiments are performed on CPU. Since JAX and TFP can run computations
on GPU, I would expect TFP to beat Numpy in the large number of samples,
number of shapes regime.</li>
<li>For Numpy and JAX the sampler is a generator and the samples are not kept in
memory. This is not the case for TFP, thus (1) the computer runs out of
memory during big experiment (2) this might affect the performance.</li>
</ol>
<p>So far, running multiple chains at once was reserved to performing posterior
checks on the convergence of algorithms, or reducing the variance (not the
biais) of the results of Monte Carlo sampling. It was traditionally achieved by
running one chain per thread on a multithreaded machine, in Python using joblib
or a custom backend. It did the job.</p>
<p>But then, multiple people started talking about vectorized sampling algorithms
that were able to generate hundreds, thousands of chains in one pass. It turns
out that I have recently gotten ridiculously obsessed with Sequential Markov
Chain Monte Carlo, and part of the requirement is to be able to sample many
chains at once. You will probably read a lot about SMCMC here in the near
future.</p>
<p>I am not going to repeat what Colin said about why we would like to have many,
many chains, so you can read the reasons here.</p>
<p>The idea was that if you want to build a reasonable PPL around this idea of
vectorized sampling, you do need a framework that performs autodiff. Tensorflow
Probability and Pyro allow you to do that already, but JAX seemed interesting to
me for several reasons:</p>
<ul>
<li>It is in most cases a drop-in replacement for numpy, and numpy is known for its
simple, clean interface (in most cases, don't jump on me here);</li>
<li>Autodiff is plain simple;</li>
<li>Its forward differentiation mode allows to easily compute higher-order
derivatives;</li>
<li>It performs JIT compilation, accelerating your code even on CPU;</li>
<li>Using GPU is straightforward;</li>
<li>Matter of taste, but I really like its functional style.</li>
</ul>
<p>Colin's MiniMC is an exercise in style, the simplest and most readable
implementation of HMC I have seen. Probably thanks to the numpy backend.
My numpy is implementation is an iteration upon his</p>
<h2 id="setup-and-results">Setup and results</h2>
<h3 id="numpy">Numpy</h3>
<p>We implement the Random Walk Metropolis algorithm in the following,
uncontroversial way:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">rw_metropolis_sampler</span>(logpdf, initial_position):
    <span style="color:#e6db74"></span><span style="color:#e6db74">&#34;&#34;&#34;</span><span style="color:#e6db74">Generate samples using the Random Walk Metropolis algorithm.</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">    Attributes</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">    ----------</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">    logpdf: function</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">      Returns the log-probability of the model given a position.</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">    initial_position: np.ndarray, shape (n_dims, n_chains)</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">      The initial position for each chain.</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">    Yields</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">    ------</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">    np.ndarray, shape (,n_chains)</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">      The next sample generated by the random walk metropolis algorithm.</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">    </span><span style="color:#e6db74">&#34;&#34;&#34;</span>
    position <span style="color:#f92672">=</span> initial_position
    log_prob <span style="color:#f92672">=</span> logpdf(initial_position)
    <span style="color:#66d9ef">yield</span> position

    <span style="color:#66d9ef">while</span> True:
        move_proposals <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0.1</span>, size<span style="color:#f92672">=</span>initial_position<span style="color:#f92672">.</span>shape)
        proposal <span style="color:#f92672">=</span> position <span style="color:#f92672">+</span> move_proposals
        proposal_log_prob <span style="color:#f92672">=</span> logpdf(proposal)

        log_unif <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>log(np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(initial_position<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], initial_position<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]))
        accept <span style="color:#f92672">=</span> log_unif <span style="color:#f92672">&lt;</span> proposal_log_prob <span style="color:#f92672">-</span> log_prob

        position <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(accept, proposal, position)
        log_prob <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(accept, proposal_log_prob, log_prob)
        <span style="color:#66d9ef">yield</span> position
</code></pre></div><h3 id="jax">JAX</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> functools <span style="color:#f92672">import</span> partial

<span style="color:#f92672">import</span> jax
<span style="color:#f92672">import</span> jax.numpy <span style="color:#f92672">as</span> np

<span style="color:#a6e22e">@partial</span>(jax<span style="color:#f92672">.</span>jit, static_argnums<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>))
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">rw_metropolis_kernel</span>(rng_key, logpdf, position, log_prob):
    move_proposals <span style="color:#f92672">=</span> jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(rng_key, shape<span style="color:#f92672">=</span>position<span style="color:#f92672">.</span>shape) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.1</span>
    proposal <span style="color:#f92672">=</span> position <span style="color:#f92672">+</span> move_proposals
    proposal_log_prob <span style="color:#f92672">=</span> logpdf(proposal)

    log_unif <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>log(jax<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(rng_key, shape<span style="color:#f92672">=</span>position<span style="color:#f92672">.</span>shape))
    accept <span style="color:#f92672">=</span> log_unif <span style="color:#f92672">&lt;</span> proposal_log_prob <span style="color:#f92672">-</span> log_prob

    position <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(accept, proposal, position)
    log_prob <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(accept, proposal_log_prob, log_prob)
    <span style="color:#66d9ef">return</span> position, log_prob


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">rw_metropolis_sampler</span>(rng_key, logpdf, initial_position):
    <span style="color:#e6db74"></span><span style="color:#e6db74">&#34;&#34;&#34;</span><span style="color:#e6db74">Vectorized Metropolis-Hastings.</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span><span style="color:#e6db74">    </span><span style="color:#e6db74">&#34;&#34;&#34;</span>
    position <span style="color:#f92672">=</span> initial_position
    log_prob <span style="color:#f92672">=</span> logpdf(initial_position)
    <span style="color:#66d9ef">yield</span> position

    <span style="color:#66d9ef">while</span> True:
        position, log_prob <span style="color:#f92672">=</span> rw_metropolis_kernel(rng_key, logpdf, position, log_prob)
        <span style="color:#66d9ef">yield</span> position
</code></pre></div><p>If you are familiar with Numpy, the syntax of the version using JAX should feel
very familiar to you. There a couple of things to note:</p>
<ol>
<li><code>jax.numpy</code> acts as a drop-in replacement to numpy. For codes only involving
array operations, replacing <code>import numpy as np</code> by <code>import jax.numpy as np</code>
should already give you performance benefits.</li>
<li>JAX handle random number generation differently from other python package,
for <a href="">very good reasons</a>. Every distribution takes a RNG key as an input.</li>
<li>We extracted the kernel from the sampler because Jax cannot compile
generators (or can it?). So we extract and JIT the function that does all the
heavy lifting: <code>rw_metropolis_kernel</code>.</li>
<li>We need to help Jax's compiler a little bit by indicating which arguments are
not going to change when the function is run several times:
<code>@partial(jax.jit, argnums=(0, 1))</code>. This is compulsory if you pass a
function as an argument, and can enable further compile-time optimizations.</li>
</ol>
<h3 id="tensorflow-probability">Tensorflow Probability</h3>
<p>For TFP we use the Random Walk Metropolis algorithm implemented in the library:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> functools <span style="color:#f92672">import</span> partial

<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">import</span> tensorflow_probability <span style="color:#f92672">as</span> tfp
tfd <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>distributions

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_raw_metropolis</span>(n_dims, n_samples, n_chains, target):
    dtype <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>float32
    samples, _ <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>mcmc<span style="color:#f92672">.</span>sample_chain(
        num_results<span style="color:#f92672">=</span>n_samples,
        current_state<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>zeros((n_chains, n_dims), dtype<span style="color:#f92672">=</span>dtype),
        kernel<span style="color:#f92672">=</span>tfp<span style="color:#f92672">.</span>mcmc<span style="color:#f92672">.</span>RandomWalkMetropolis(target<span style="color:#f92672">.</span>log_prob, seed<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>),
        num_burnin_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,
        parallel_iterations<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>,
    )

    <span style="color:#66d9ef">return</span> samples

run_mcm <span style="color:#f92672">=</span> partial(run_tfp_mcmc, n_dims, n_samples, n_chains, target)

<span style="color:#75715e">## Without XLA</span>
run_mcm()

<span style="color:#75715e">## With XLA compilation</span>
tf<span style="color:#f92672">.</span>xla<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>compile(run_mcm)
</code></pre></div><p><a href="/static/img/rw_benchmark/samples.png">Samples</a>
<a href="/static/img/rw_benchmark/chains.png">Chains</a></p>
<p>Numpy was a serious contender of Jax for RW Metropolis. However, there is one
place where it cannot compete, situations where the gradient of the function is
needed. Hamiltonian Monte Carlo!</p>
<p>TFP must be doing some sort of step-size adaptation which may affect the
performance in terms on time (but not sample quality).</p>
<p>Better measure is Effective Sample / s but here we are just comaring raw
performance. The numbers may shift, but the comparisons should still hold.</p>


  </article>


      <footer class="site-footer">
        <span itemscope itemtype="http://schema.org/Person">
          <link itemprop="url" href="http://rlouf.github.com/">
          <span itemprop="name"></span>

          <br>

          <a itemprop="sameAs" href="https://github.com/rlouf" title="GitHub">Github</a>

          <a itemprop="sameAs" href="https://twitter.com/remilouf" title="Twitter">Twitter</a>

          
        </span>

        
      </footer>
    </div>

  <script src="/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  </body>
</html>

