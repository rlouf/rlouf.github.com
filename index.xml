<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>/dev/null</title>
    <link>http://rlouf.github.com/</link>
    <description>Recent content on /dev/null</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Sep 2020 19:09:52 +0200</lastBuildDate>
    
	<atom:link href="http://rlouf.github.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Introducing_mcx</title>
      <link>http://rlouf.github.com/post/introduce_mcx/</link>
      <pubDate>Fri, 25 Sep 2020 19:09:52 +0200</pubDate>
      
      <guid>http://rlouf.github.com/post/introduce_mcx/</guid>
      <description>I have been working on a personal project since last February (remember, before the pandemic). I had two goals in mind at the time: learn JAX, and learn how to code a PPL. I had been using Stan and PyMC3 for years and felt some frustration at not having a good grasp of how inference was happening under the hood. This was by no means intended to be a PPL at the time, more like Colin Carrol&amp;rsquo;s MiniMC but using JAX for performance.</description>
    </item>
    
    <item>
      <title>Designing modular inference engines: API for the HMC kernel</title>
      <link>http://rlouf.github.com/post/ppl_integrator_api/</link>
      <pubDate>Thu, 13 Feb 2020 09:13:38 +0100</pubDate>
      
      <guid>http://rlouf.github.com/post/ppl_integrator_api/</guid>
      <description>I have been working on a probabilistic programming library, MCX (don&amp;rsquo;t use it yet, most of the inference engine is in API prototype stage) for the past few weeks. I will write more about it soon, but the library is based on source code transformation: you express the model as a python function, and a compiler reads the function, applies the necessary transformations and outputs either a function that generates samples from this distribution, or its logpdf.</description>
    </item>
    
    <item>
      <title>Massively parallel MCMC with JAX</title>
      <link>http://rlouf.github.com/post/jax-random-walk-metropolis/</link>
      <pubDate>Thu, 09 Jan 2020 13:19:11 +0100</pubDate>
      
      <guid>http://rlouf.github.com/post/jax-random-walk-metropolis/</guid>
      <description>TL;DR Edit on 2020/10/01: As pointed out by Matthew Johnson and Hector Yee, the results reported in a previous version of this post were artificially biaised in favor of JAX due to my code not &amp;ldquo;advancing&amp;rdquo; the random number generator. I updated all curves and numbers, and added a little word of caution regarding the use of JAX&amp;rsquo;s pseudo-random number generator.
The code necessary to reproduce the results can be found here.</description>
    </item>
    
    <item>
      <title>The Engine: a framework for growth</title>
      <link>http://rlouf.github.com/post/growth-model/</link>
      <pubDate>Mon, 05 Nov 2018 22:00:10 +0100</pubDate>
      
      <guid>http://rlouf.github.com/post/growth-model/</guid>
      <description>A popular set of metrics against which growth of applications can be measured is the &amp;ldquo;pirate metrics&amp;rdquo;, or AARRR metrics:
 Acquisition: the number of users who get in your application every day; Activation: the proportion of these users who perform a meaningful action. For a music streaming app it could be to play at least one song entirely. For a food delivery service, placing an order; Retention: the proportion of users who are still there after 1, 7, 30 days, etc1; Referral: The proportion of users who refer the application to potential users; Revenue: the proportion of users who pay for your service.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://rlouf.github.com/post/mcx_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://rlouf.github.com/post/mcx_api/</guid>
      <description>Minimal coupling I like to think of mcx as being the composition of three loosely coupled libraries rather than a full library:
 The core, which allows to build graphical models and compile them to a forward sampling function or a log-probability density function; The distributions which implement, well, distribution. Most of the heavy lifting will be transfered to JAX&amp;rsquo;s scipy and numpy in a near future. We just add utilities to check the parameters&amp;rsquo; domain and ensure that value stay in the support; The inference engine, a collection of momentum generators/kinetic energies, integrators, adaptive schemes that can be combined into warmup and inference kernels.</description>
    </item>
    
  </channel>
</rss>